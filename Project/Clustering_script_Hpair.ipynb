{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dad668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from csv import DictReader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_mutual_info_score, homogeneity_score, rand_score, adjusted_rand_score, pair_confusion_matrix\n",
    "import random\n",
    "import time\n",
    "from igraph import *\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from scipy.stats import entropy\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57023ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"./data/edgeData\").iterdir()\n",
    "\n",
    "############# OUR CODE BEGINS ##############\n",
    "\n",
    "files = [x for x in p if x.is_file() and \"edges_2012_2\" in x.stem or \"edges_2014_2\" in x.stem]\n",
    "heuristics = [\"h0\",\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]\n",
    "\n",
    "############# OUR CODE ENDS ##############\n",
    "\n",
    "for file_name in files:\n",
    "\n",
    "        print('file_name: ',file_name)\n",
    "\n",
    "        df = pd.read_csv(file_name)\n",
    "\n",
    "        ############# OUR CODE BEGINS ##############\n",
    "        \n",
    "\n",
    "        for h in heuristics:\n",
    "            \n",
    "            if h == \"h0\":\n",
    "                df[\"sum\"] = df[[\"h0\"]]\n",
    "                name = f\"h0\"\n",
    "                \n",
    "            else:\n",
    "                df[\"sum\"] = df[[\"h0\", h]].sum(axis=1)\n",
    "                name = f\"h0_{h}\"\n",
    "            \n",
    "            \n",
    "            ############# OUR CODE ENDS ##############\n",
    "\n",
    "            df_gt = pd.read_csv('data/ground_truth_id.csv.zip')\n",
    "            # print(df_gt.shape)\n",
    "\n",
    "            df_gt = df_gt.dropna(axis=0, how='any', subset=[\"address\", \"entity\"])\n",
    "            # print(df_gt.shape)\n",
    "\n",
    "            df_gt = df_gt.drop_duplicates(subset=\"address\", keep=False)\n",
    "            # print(df_gt.shape)\n",
    "\n",
    "            entity_counts = df_gt[\"entity\"].value_counts()\n",
    "            rare_entities = entity_counts[entity_counts < 10].index\n",
    "            df_gt = df_gt.loc[~df_gt[\"entity\"].isin(rare_entities), :]\n",
    "            # print(df_gt.shape)\n",
    "\n",
    "            gt_addr = set(df_gt[\"address\"])\n",
    "            sample_addr = set(df[\"node1\"]).union(set(df[\"node2\"]))\n",
    "            sample_known_addr = sample_addr.intersection(gt_addr)\n",
    "\n",
    "            # print('ground truth addresses',len(gt_addr))\n",
    "            # print('sample addressses:',len(sample_addr))\n",
    "\n",
    "            df_gt_known = df_gt.loc[df_gt[\"address\"].isin(sample_addr), ['address', 'entity']]\n",
    "\n",
    "            known_entities = set(df_gt_known['entity'])\n",
    "\n",
    "            # known_entity_addr_dict = {idx: set(df_gt_known.loc[df_gt_known[\"entity\"] == e, \"address\"]) for idx, e in enumerate(known_entities)}\n",
    "\n",
    "            known_addr_entity_dict_list = [{a: idx for a in set(df_gt_known.loc[df_gt_known[\"entity\"] == e, \"address\"])} for idx, e in enumerate(known_entities)]\n",
    "\n",
    "            #print('known_addr_entity_dict_list: ', known_addr_entity_dict_list)\n",
    "\n",
    "            known_addr_entity_dict = {}\n",
    "            [known_addr_entity_dict.update(d) for d in known_addr_entity_dict_list]\n",
    "            known_entity_counts = df_gt_known['entity'].value_counts().rename(\"count\").to_frame()\n",
    "\n",
    "            # print('ground truth entities in sample:',len(known_entities))\n",
    "\n",
    "            known_entity_counts[\"file\"] = file_name.stem\n",
    "            known_entity_counts[\"n_nodes_graph\"] = len(sample_addr)\n",
    "            known_entity_counts[\"n_edges_graph\"] = len(df)\n",
    "\n",
    "            now = datetime.now()\n",
    "            date_str = now.strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "            file_name_output = f\"known_entity_counts_{file_name.stem}_res_{date_str}.csv\"\n",
    "            output_path = Path(f\"data/res/{file_name_output}\")\n",
    "\n",
    "            #print('output_path: ', output_path)\n",
    "\n",
    "            known_entity_counts.to_csv(output_path, index=True)\n",
    "\n",
    "            def get_labels(cs_addr, known_addr_entity_dict):\n",
    "                    res = {}\n",
    "                    for idx, c in enumerate(cs_addr):\n",
    "                            for a in c:\n",
    "                                    if a in sample_known_addr:\n",
    "                                            res[a] = idx\n",
    "\n",
    "                    labels_true = []\n",
    "                    labels_pred = []\n",
    "\n",
    "                    for a in res.keys():\n",
    "                            labels_true.append(known_addr_entity_dict[a])\n",
    "                            labels_pred.append(res[a])\n",
    "\n",
    "                    return labels_true, labels_pred\n",
    "\n",
    "            edge_tuples = df[[\"node1\", \"node2\", \"sum\"]].itertuples(index=False)\n",
    "            g = Graph.TupleList(edge_tuples, directed=False, weights=True)\n",
    "            g_weights = g.es[\"weight\"]\n",
    "\n",
    "            def get_initial_and_fixed(sample_size, seed=32512163):\n",
    "                    known_idxs = [idx for idx, addr in enumerate(g.vs[\"name\"]) if addr in sample_known_addr]\n",
    "\n",
    "                    initial = np.zeros(len(g.vs), dtype=np.int)\n",
    "\n",
    "\n",
    "                    rng = np.random.RandomState(seed)\n",
    "\n",
    "                    sample_known_idxs = rng.choice(known_idxs, size=sample_size, replace=False) # TODO: set seed\n",
    "                    non_sample_idxs = np.setdiff1d(range(len(g.vs)), sample_known_idxs)\n",
    "\n",
    "                    for idx in sample_known_idxs:\n",
    "                            initial[idx] = known_addr_entity_dict[g.vs[idx][\"name\"]]\n",
    "\n",
    "                    available_labels = np.setdiff1d(range(len(g.vs)), initial[sample_known_idxs])[:len(g.vs) - sample_size]\n",
    "\n",
    "                    initial[non_sample_idxs] = available_labels\n",
    "\n",
    "                    fixed = np.zeros(len(g.vs), dtype=np.int)\n",
    "                    fixed[sample_known_idxs] = 1\n",
    "                    fixed = fixed.astype(bool)\n",
    "\n",
    "                    return initial, fixed\n",
    "\n",
    "\n",
    "            props = [0, 0.1] + list(np.geomspace(start=0.01, stop=0.4, num=15)) # np.arange(0, 1, 0.05)\n",
    "            props = sorted(props)\n",
    "            # print(props)\n",
    "            # print()\n",
    "\n",
    "            sizes = [int(p * len(g.vs)) for p in props if int(p * len(g.vs)) <= len(sample_known_addr)]\n",
    "            #print(sizes)\n",
    "\n",
    "            for seed in range(101):\n",
    "                total_rnd_iter = 1\n",
    "                for rnd_iter in range(total_rnd_iter):\n",
    "                        print(f\"{rnd_iter + 1} / {total_rnd_iter}\")\n",
    "\n",
    "                        if rnd_iter > 0:\n",
    "                                print(\"randomizing...\")\n",
    "                                g.rewire(n=4*len(g.es), mode=\"simple\")\n",
    "                                print(\"done rewiring\")\n",
    "                                g.es[\"weight\"] = g_weights\n",
    "                                print(\"randomized\")\n",
    "\n",
    "                        res_cols = [\"file\", \"n_nodes_graph\", \"n_edges_graph\", \"prop_graph\", \"prop_known\",\n",
    "                                                \"n_clusters\", \"cluster_sizes\", \"ami\", \"homog\", \"mod\", \"ars\", \"urs\",\n",
    "                                                # \"entropy_vs_cluster_entropy\", \n",
    "                                                # \"entropy_vs_cluster_sizes_known_addr\", \n",
    "                                                # \"entropy_vs_cluster_n_entities\", \n",
    "                                                # \"entropy_vs_cluster_sizes_labels_true\", \n",
    "                                                # \"entropy_vs_entity_entropy\",\n",
    "                                                # \"entropy_vs_entity_abs_size\",\n",
    "                                                # \"entropy_vs_entity_rel_size\"\n",
    "                                                ]\n",
    "\n",
    "                        res = {c: [] for c in res_cols}\n",
    "\n",
    "                        for i, size in enumerate(sizes):\n",
    "                                initial, fixed = get_initial_and_fixed(size, seed=seed)\n",
    "                                cs = g.community_label_propagation(weights=\"weight\", initial=initial, fixed=fixed)\n",
    "\n",
    "                                cs_addr = sorted([g.vs.select(c)[\"name\"] for c in cs], key=len)\n",
    "                                cs_sizes = [len(c) for c in cs_addr]\n",
    "\n",
    "                                labels_true, labels_pred = get_labels(cs_addr=cs_addr, known_addr_entity_dict=known_addr_entity_dict)\n",
    "\n",
    "                                ami = adjusted_mutual_info_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "                                urs = rand_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "\n",
    "                                # ars = adjusted_rand_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "                                (tn, fp), (fn, tp) = pair_confusion_matrix(labels_true, labels_pred)\n",
    "\n",
    "                                if fn == 0 and fp == 0:\n",
    "                                        ars = 1.0\n",
    "                                else:\n",
    "                                        ars = 2. * (tp.astype(np.float64) * tn.astype(np.float64) - fn.astype(np.float64) * fp.astype(np.float64)) / ((tp.astype(np.float64) + fn.astype(np.float64)) * (fn.astype(np.float64) + tn.astype(np.float64)) + (tp.astype(np.float64) + fp.astype(np.float64)) * (fp.astype(np.float64) + tn.astype(np.float64)))\n",
    "\n",
    "                                homog = homogeneity_score(labels_true=labels_true, labels_pred=labels_pred)\n",
    "                                mod = cs.modularity\n",
    "\n",
    "                                res[\"file\"].append(file_name.stem)\n",
    "                                res[\"n_nodes_graph\"].append(len(sample_addr))\n",
    "                                res[\"n_edges_graph\"].append(len(df))\n",
    "                                res[\"prop_graph\"].append(props[i])\n",
    "                                res[\"prop_known\"].append(size/len(sample_known_addr))\n",
    "                                res[\"n_clusters\"].append(len(cs))\n",
    "                                res[\"cluster_sizes\"].append(cs_sizes)\n",
    "                                res[\"ami\"].append(ami)\n",
    "                                res[\"ars\"].append(ars)\n",
    "                                res[\"urs\"].append(urs)\n",
    "                                res[\"homog\"].append(homog)\n",
    "                                res[\"mod\"].append(mod)\n",
    "\n",
    "                                print(f\"{props[i]:.2%} \\t {size/len(sample_known_addr):.2%}   \\t ami: {ami:.2f} \\t urs: {urs:.2f} \\t ars: {ars:.2f} \\t homog: {homog:.2f} \\t {mod:.2f}\")\n",
    "\n",
    "                                continue\n",
    "\n",
    "                                entropy_vs_entity_entropy = []\n",
    "                                entropy_vs_entity_abs_size = []\n",
    "                                entropy_vs_entity_rel_size = []\n",
    "\n",
    "                                entity_cluster_distribution_dict = defaultdict(lambda: defaultdict(int))\n",
    "                                for entity, cluster in zip(labels_true, labels_pred):\n",
    "                                        entity_cluster_distribution_dict[entity][cluster] += 1\n",
    "\n",
    "                                for entity, cluster_dist in entity_cluster_distribution_dict.items():\n",
    "                                        e = entropy(list(cluster_dist.values()), base=2)\n",
    "                                        entropy_vs_entity_entropy.append(e)\n",
    "                                        entropy_vs_entity_abs_size.append(sum([v for v in cluster_dist.values()]))\n",
    "                                        entropy_vs_entity_rel_size.append(sum([v for v in cluster_dist.values()]) / len(sample_known_addr))\n",
    "\n",
    "                                res[\"entropy_vs_entity_entropy\"].append(entropy_vs_entity_entropy)\n",
    "                                res[\"entropy_vs_entity_abs_size\"].append(entropy_vs_entity_abs_size)\n",
    "                                res[\"entropy_vs_entity_rel_size\"].append(entropy_vs_entity_rel_size)\n",
    "\n",
    "                                n_entities = []\n",
    "                                entropys = []\n",
    "                                cluster_sizes_known_addr = []\n",
    "                                cluster_sizes_labels_true = []\n",
    "                                for c in cs_addr:\n",
    "                                        labels_true, labels_pred = get_labels(cs_addr=[c], known_addr_entity_dict=known_addr_entity_dict)\n",
    "\n",
    "                                        if len(labels_pred) == 0:\n",
    "                                                continue\n",
    "\n",
    "                                        _, counts = np.unique(labels_true, return_counts=True)\n",
    "\n",
    "                                        e = entropy(counts, base=2)\n",
    "                                        entropys.append(e)\n",
    "\n",
    "                                        n_entity = len(np.unique(labels_true))\n",
    "                                        n_entities.append(n_entity)\n",
    "\n",
    "                                        cluster_sizes_known_addr.append(len(labels_true))\n",
    "                                        cluster_sizes_labels_true.append(len(c))\n",
    "\n",
    "                                res[\"entropy_vs_cluster_n_entities\"].append(n_entities)\n",
    "                                res[\"entropy_vs_cluster_entropy\"].append(entropys)\n",
    "                                res[\"entropy_vs_cluster_sizes_known_addr\"].append(cluster_sizes_known_addr)\n",
    "                                res[\"entropy_vs_cluster_sizes_labels_true\"].append(cluster_sizes_labels_true)\n",
    "\n",
    "                        df_res = pd.DataFrame(res)\n",
    "                        #print(df_res.info())\n",
    "\n",
    "                        now = datetime.now()\n",
    "                        date_str = now.strftime(\"%Y%m%d_%H%M_%s\")\n",
    "\n",
    "                        ############# OUR CODE STARTS ##############\n",
    "                        \n",
    "                        #file_name_output = f\"{file_name.stem}_{seed}_res_{date_str}.csv\"\n",
    "                        file_name_output = f\"{file_name.stem}_${name}$_{seed}_res_{date_str}.csv\"\n",
    "                        \n",
    "                        ############# OUR CODE ENDS ###############\n",
    "                        \n",
    "                        output_path = Path(f\"data/res/{file_name_output}\")\n",
    "\n",
    "                        df_res.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a3f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
